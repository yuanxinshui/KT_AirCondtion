{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 训练模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "#import pymysql\n",
    "#pymysql.install_as_MySQLdb()\n",
    "import MySQLdb as sql\n",
    "import sklearn.mixture as mix\n",
    "\n",
    "import math\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "#from pyecharts import Line\n",
    "\n",
    "#不输出警告\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import time\n",
    "import datetime\n",
    "import pickle\n",
    "from sklearn.externals import joblib\n",
    "\n",
    "filesave=os.getcwd()+r'\\online_test'\n",
    "\n",
    "#0号门模型\n",
    "a_open_sd_model_flie=r'\\onlycan_opendata_sd_model0.pkl'\n",
    "a_close_sd_model_flie=r'\\onlycan_closedata_sd_model0.pkl'\n",
    "a_open_normal_model_flie=r'\\onlycan_opendata_normal_obj_model0.pkl'\n",
    "a_close_normal_model_flie=r'\\onlycan_closedata_normal_obj_model0.pkl'\n",
    "#1号门模型\n",
    "b_open_sd_model_flie=r'\\onlycan_opendata_sd_model1.pkl'\n",
    "b_close_sd_model_flie=r'\\onlycan_closedata_sd_model1.pkl'\n",
    "b_open_normal_model_flie=r'\\onlycan_opendata_normal_obj_model1.pkl'\n",
    "b_close_normal_model_flie=r'\\onlycan_closedata_normal_obj_model1.pkl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\\\online_test\\\\onlycan_opendata_sd_model0.pkl'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filesave+a_open_sd_model_flie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import threading\n",
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_gmm(x):\n",
    "    if x > 0.5:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#计算CV值\n",
    "def calculate_cv(MG1,MG2):\n",
    "\n",
    "    \n",
    "    NumMixture1 = MG1.n_components\n",
    "    NumMixture2 = MG2.n_components\n",
    "    d = 0.0\n",
    "    for k in range(0,NumMixture1):\n",
    "        for j in range(0,NumMixture2):\n",
    "            Sigma1 = MG1.covariances_[k]\n",
    "            Mu1 = MG1.means_[k]\n",
    "            Sigma2 = MG2.covariances_[j]\n",
    "            Mu2 = MG2.means_[j]\n",
    "            weight = MG1.weights_[k] * MG2.weights_[j]\n",
    "            dist = L2Dist(Sigma1,Mu1,Sigma2,Mu2)\n",
    "            d = d + weight*dist\n",
    "    ''' the L2 norm of each GMM model'''\n",
    "    n1 = 0.0\n",
    "    n2 = 0.0\n",
    "    for k in range(0,NumMixture1):\n",
    "        for j in range(0,NumMixture1):\n",
    "            Sigma1 = MG1.covariances_[k]\n",
    "            Mu1 = MG1.means_[k]\n",
    "            Sigma2 = MG1.covariances_[j]\n",
    "            Mu2 = MG1.means_[j]\n",
    "            weight = MG1.weights_[k]*MG1.weights_[j]\n",
    "            n1 = n1 + weight * L2Dist(Sigma1,Mu1,Sigma2,Mu2)\n",
    "    for k in range(0,NumMixture2):\n",
    "        for j in range(0,NumMixture2):\n",
    "            Sigma1 = MG2.covariances_[k]\n",
    "            Mu1 = MG2.means_[k]\n",
    "            Sigma2 = MG2.covariances_[j]\n",
    "            Mu2 = MG2.means_[j]\n",
    "            weight = MG2.weights_[k]*MG2.weights_[j]\n",
    "            n2 = n2 + weight * L2Dist(Sigma1,Mu1,Sigma2,Mu2)\n",
    "    cv = d/np.sqrt(n1)/np.sqrt(n2)\n",
    "    return cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#计算两个高斯分布之间的距离\n",
    "def L2Dist(sigma1,mean1,sigma2,mean2):\n",
    "    '''\n",
    "    calculate the L2 distance between two multivariate Gaussian distributisons\n",
    "    \n",
    "    '''    \n",
    "    n = mean1.size\n",
    "    \n",
    "    '''L2 norm of sigma1, mean1'''\n",
    "    \n",
    "    invsigma1 = np.linalg.inv(sigma1)\n",
    "    invsigma2 = np.linalg.inv(sigma2)\n",
    "    u = invsigma1*mean1 + invsigma2*mean2\n",
    "    invsigma = np.linalg.inv(sigma1+sigma2)\n",
    "    f1norm = 1/math.sqrt( math.sqrt( (2*math.pi)**n*np.linalg.det(sigma1)*2**n ) )\n",
    "    '''L2 norm of sigma2, mean2'''\n",
    "    f2norm = 1/math.sqrt( math.sqrt( (2*math.pi)**n*np.linalg.det(sigma2)*2**n ) )\n",
    "    '''inner product of pdf f1 and f2'''\n",
    "    \n",
    "    InnerProd = 1/(math.sqrt(2*math.pi)**n) * math.sqrt(np.linalg.det(invsigma)) * \\\n",
    "                np.exp(-1/2*(-u.T*sigma1*invsigma*sigma2*u + mean1.T*invsigma1*mean1 + mean2.T*invsigma2*mean2))\n",
    "    '''L2 distance between f1 and f2'''\n",
    "    d = InnerProd/f1norm/f2norm\n",
    "    \n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#训练高斯模型\n",
    "def gmm_clustering(k,X,cov_type=None):\n",
    "    '''\n",
    "    基于高斯混合模型GMM进行无监督学习，聚类\n",
    "    其中GMM采用传统的EM算法进行参数估计\n",
    "    \n",
    "    输入参数含义:\n",
    "    k    ————   聚类的数目，必填项\n",
    "    X    ————   样本的输入，比填项\n",
    "    cov_type  ————  协方差类型，可选项\n",
    "            四种选择：'spherical', 'tied', 'diag', 'full'\n",
    "            如果不填写，则默认为'full'\n",
    "        \n",
    "    '''\n",
    "    if cov_type is None:\n",
    "        cov_type = 'full'   \n",
    "    \n",
    "    gmm = mix.GaussianMixture(n_components=k,covariance_type=cov_type)\n",
    "    gmm.fit(X)\n",
    "    \n",
    "    return gmm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 去除离群点"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#去除异常数据规则：不满足均值 正负 3simga范围内的数据，均不保留\n",
    "def filter_data(data):\n",
    "    col =data.shape[1]\n",
    "    for i in range(col):\n",
    "        min_value = data.iloc[:,i].mean()-3*data.iloc[:,i].std()\n",
    "        max_value = data.iloc[:,i].mean()+3*data.iloc[:,i].std()\n",
    "        data.iloc[:,i][(data.iloc[:,i]>max_value) |(data.iloc[:,i]<min_value)] = np.nan\n",
    "    data=data.dropna()#删除缺失数据\n",
    "    return data\n",
    "# filter_df=filter_data(df)\n",
    "# filter_df=rawdata.ix[filter_df.index,:]\n",
    "# filter_df=rawdata_can_open_filter.dropna()#删除缺失数据"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 开关门数据标准化函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def open_StandardScaler(data_can_open,open_sd_model_flie):\n",
    "    #去除离群点后的数据\n",
    "    data_st1_sps = data_can_open.copy()\n",
    "    #使用去除离群点的数据训练归一化模型\n",
    "    #用去除故障点的数据进行训练\n",
    "    standardscaler_st1 = preprocessing.StandardScaler()\n",
    "    standardscaler_st1.fit(data_st1_sps)\n",
    "\n",
    "\n",
    "    #用该模型将去除离群点和故障的数据进行标准化\n",
    "    data_st1_sd_sps = standardscaler_st1.transform(data_st1_sps)\n",
    "    data_st1_sd_sps = pd.DataFrame(data_st1_sd_sps)\n",
    "    data_st1_sd_sps.index=data_st1_sps.index\n",
    "    data_st1_sd_sps.columns=data_st1_sps.columns\n",
    "\n",
    "###############################################################################    \n",
    "    #保存开门数据标准化模型\n",
    "    #保存训练好的模型文件\n",
    "    joblib.dump(standardscaler_st1, filesave+open_sd_model_flie,compress=3)\n",
    "    #导入训练好的模型文件\n",
    "    #clf = joblib.load('onlycan_opendata_sd_model0.pkl') \n",
    "###############################################################################  \n",
    "    return data_st1_sd_sps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def close_StandardScaler(data_can_close,close_sd_model_flie):\n",
    "    #去除离群点后的数据\n",
    "    data_st2_sps = data_can_close.copy()\n",
    "\n",
    "    #使用去除离群点的数据训练归一化模型\n",
    "    #用去除故障点的数据进行训练\n",
    "    standardscaler_st2 = preprocessing.StandardScaler()\n",
    "    standardscaler_st2.fit(data_st2_sps)\n",
    "\n",
    "    #用该模型将去除离群点和故障的数据进行标准化\n",
    "    data_st2_sd_sps = standardscaler_st2.transform(data_st2_sps)\n",
    "    data_st2_sd_sps = pd.DataFrame(data_st2_sd_sps)\n",
    "    data_st2_sd_sps.index=data_st2_sps.index\n",
    "    data_st2_sd_sps.columns=data_st2_sps.columns\n",
    "###############################################################################    \n",
    "    #保存关门数据标准化模型\n",
    "    #保存训练好的模型文件\n",
    "    joblib.dump(standardscaler_st2, filesave+close_sd_model_flie,compress=3)\n",
    "    #导入训练好的模型文件\n",
    "#     clf = joblib.load('onlycan_closedata_sd_model0.pkl')\n",
    "###############################################################################  \n",
    "    return data_st2_sd_sps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 开门数据寻优函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def opendata_findbest(data_st1_sd_sps,open_normal_model_flie):\n",
    "    time_start=time.time()\n",
    "    #找出表现最好的行编号---开门状态\n",
    "    #选取前50个点，计算cv均值\n",
    "    list_findopenbest=[]\n",
    "    sample_len=50\n",
    "    print('open_sample_len',sample_len)\n",
    "    i=0\n",
    "    while len(list_findopenbest)!=sample_len:\n",
    "        row_num=i\n",
    "        i+=1\n",
    "        k = 1\n",
    "        cv1 = np.zeros(1)\n",
    "        data_so = data_st1_sd_sps.copy()    \n",
    "        data_s1 = data_so.iloc[row_num].copy()\n",
    "        data_s1 = pd.DataFrame(data_s1)\n",
    "        #print(data_so.index[row_num])\n",
    "\n",
    "        data_s = data_st1_sd_sps.copy()\n",
    "\n",
    "        normal_obj = gmm_clustering(k,data_s1)\n",
    "        a = len(data_s)\n",
    "        cv = np.zeros(a)\n",
    "\n",
    "        #建立高斯混合模型\n",
    "        def gmm_apply(x,k,normal_obj):\n",
    "            data_l = pd.DataFrame(x)\n",
    "            test_obj = gmm_clustering(k,data_l)\n",
    "            y = calculate_cv(normal_obj,test_obj)#y为[[cv值]]，只返回cv值\n",
    "            return y[0][0]\n",
    "\n",
    "        #计算每一行的cv值，存在data_s的cv列中         \n",
    "        data_s['cv']=data_s.apply(lambda row:gmm_apply(row,k,normal_obj), axis=1)\n",
    "\n",
    "        #计算cv的均值\n",
    "        cv1= data_s['cv']#cv1为Series\n",
    "        #print(type(cv1))\n",
    "        count =0    \n",
    "        data_s['health_label']=cv1.map(count_gmm) \n",
    "        count=data_s['health_label'].sum()   \n",
    "        c = cv1.mean() \n",
    "        ##########################################\n",
    "        list_row=[data_so.index[row_num],a,count,c]\n",
    "        list_findopenbest.append(list_row)\n",
    "\n",
    "    df_findopenbest=pd.DataFrame(list_findopenbest,columns=['time','a', 'count', 'cv.mean'] )   \n",
    "    #用cv.mean列对df进行排序\n",
    "    df_openbest=df_findopenbest.sort_values(by=\"cv.mean\" , ascending=False)#按照降序排序\n",
    "\n",
    "    time_end=time.time()\n",
    "    print('findopenbest totally cost',datetime.timedelta(seconds=time_end-time_start))\n",
    "    print('df_openbest',df_openbest.head())\n",
    "##########################################\n",
    "    #保存效果最好的模型\n",
    "    data_so = data_st1_sd_sps.copy()\n",
    "    best_time_df=df_openbest.iloc[[0],[0]].values\n",
    "    print(best_time_df)\n",
    "    for i in best_time_df:\n",
    "        for j in i:\n",
    "            best_time=j        \n",
    "    print(best_time)\n",
    "    data_s1 = data_so.loc[best_time].copy()\n",
    "    data_s1 = pd.DataFrame(data_s1)\n",
    "    normal_obj = gmm_clustering(1,data_s1)\n",
    "    #保存normal_obj模型\n",
    "    #保存训练好的模型文件\n",
    "    joblib.dump(normal_obj, filesave+open_normal_model_flie,compress=3)\n",
    "    #导入训练好的模型文件\n",
    "#     clf = joblib.load('onlycan_opendata_normal_obj_model0.pkl') \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 关门数据寻优函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def closedata_findbest(data_st2_sd_sps,close_normal_model_flie):\n",
    "    time_start=time.time()\n",
    "    #找出表现最好的行编号---关门状态\n",
    "    #选取前50个点，计算cv均值\n",
    "    list_findclosebest=[]\n",
    "    sample_len=50\n",
    "    print('close_sample_len',sample_len)\n",
    "    i=0\n",
    "    while len(list_findclosebest)!=sample_len:\n",
    "        row_num=i\n",
    "        i+=1\n",
    "        k = 1\n",
    "        cv1 = np.zeros(1)\n",
    "        data_so = data_st2_sd_sps.copy()    \n",
    "        data_s1 = data_so.iloc[row_num].copy()\n",
    "        data_s1 = pd.DataFrame(data_s1)\n",
    "        \n",
    "        data_s = data_st2_sd_sps.copy()\n",
    "\n",
    "        normal_obj = gmm_clustering(k,data_s1)\n",
    "        a = len(data_s)\n",
    "        cv = np.zeros(a)\n",
    "\n",
    "        #建立高斯混合模型\n",
    "        def gmm_apply(x,k,normal_obj):\n",
    "            data_l = pd.DataFrame(x)\n",
    "            test_obj = gmm_clustering(k,data_l)\n",
    "            y = calculate_cv(normal_obj,test_obj)#y为[[cv值]]，只返回cv值\n",
    "            return y[0][0]\n",
    "\n",
    "        #计算每一行的cv值，存在data_s的cv列中         \n",
    "        data_s['cv']=data_s.apply(lambda row:gmm_apply(row,k,normal_obj), axis=1)\n",
    "\n",
    "        #计算cv的均值\n",
    "        cv1= data_s['cv']#cv1为Series\n",
    "        #print(type(cv1))\n",
    "        count =0    \n",
    "        data_s['health_label']=cv1.map(count_gmm) \n",
    "        count=data_s['health_label'].sum()   \n",
    "        c = cv1.mean() \n",
    "        ##########################################\n",
    "        list_row=[data_so.index[row_num],a,count,c]\n",
    "        list_findclosebest.append(list_row)\n",
    "\n",
    "    df_findclosebest=pd.DataFrame(list_findclosebest,columns=['time','a', 'count', 'cv.mean'] )   \n",
    "    #用cv.mean列对df进行排序\n",
    "    df_closebest=df_findclosebest.sort_values(by=\"cv.mean\" , ascending=False)#按照降序排序\n",
    "\n",
    "    time_end=time.time()\n",
    "    print('findclosebest totally cost',datetime.timedelta(seconds=time_end-time_start))\n",
    "    print('df_closebest',df_closebest.head())\n",
    "##########################################\n",
    "    #保存效果最好的模型\n",
    "    data_so = data_st2_sd_sps.copy()\n",
    "    best_time_df=df_closebest.iloc[[0],[0]].values\n",
    "    for i in best_time_df:\n",
    "        for j in i:\n",
    "            best_time=j        \n",
    "    print(best_time)\n",
    "    data_s1 = data_so.loc[best_time].copy()\n",
    "#     data_s1 = data_so.loc[df_closebest.ix[[0],['time']]].copy()\n",
    "    data_s1 = pd.DataFrame(data_s1)\n",
    "    normal_obj = gmm_clustering(1,data_s1)\n",
    "    #保存normal_obj模型\n",
    "    #保存训练好的模型文件\n",
    "    joblib.dump(normal_obj, filesave+close_normal_model_flie,compress=3)\n",
    "    #导入训练好的模型文件\n",
    "#     clf = joblib.load('onlycan_closedata_normal_obj_model0.pkl') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 自动定位寻优区间"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def findbestperiod(rawdata_can_open):\n",
    "    #当doortime前两位一致的时候对em进行排序,万一一天运行出的数据不足500条怎么办\n",
    "    time_start=time.time()\n",
    "    period=0\n",
    "    findperiod_list=[]\n",
    "    \n",
    "    DOOR_STATE_TIME_mode=rawdata_can_open['DOOR_STATE_TIME'].mode()[0]#众数\n",
    "    print('DOOR_STATE_TIME_mode',DOOR_STATE_TIME_mode)\n",
    "    oneday=datetime.timedelta(days=1) #1天，用于+1天操作\n",
    "    can_date=rawdata_can_open.index[0].date()\n",
    "    end_can_date=rawdata_can_open.index[-1].date()\n",
    "    #print (end_can_date)\n",
    "    #can_date=datetime.datetime.strptime(can_date_str, '%Y-%m-%d')#从str格式转回datetime\n",
    "    \n",
    "    while(1):\n",
    "        #print (can_date)\n",
    "        if can_date==end_can_date+oneday:#超过最后的日期\n",
    "            break\n",
    "        else:\n",
    "            can_date_str=can_date.strftime('%Y-%m-%d')#转化为字符串\n",
    "            oneday_data=rawdata_can_open.ix[can_date_str].copy()\n",
    "            #print (len(oneday_data))\n",
    "            if len(oneday_data)>100:#判断标准化需要的数据足够\n",
    "\n",
    "                door_state_time_count=int(100*len(oneday_data[oneday_data['DOOR_STATE_TIME']==DOOR_STATE_TIME_mode])/len(oneday_data))#计算该天内DOOR_STATE_TIME值等于众数的百分比\n",
    "                em_mean=abs(oneday_data['EM'].mean())\n",
    "                list_row=[can_date_str,door_state_time_count,em_mean]\n",
    "                findperiod_list.append(list_row)\n",
    "                can_date=can_date+oneday#加一天  \n",
    "            else:\n",
    "                can_date=can_date+oneday#加一天\n",
    "\n",
    "\n",
    "    df_findperiod=pd.DataFrame(findperiod_list,columns=['time','door_state_time_count', 'EM_mean'] )   \n",
    "    #对door_state_time_count占比达标的period根据em进行升序排序\n",
    "    df_findperiod=df_findperiod[df_findperiod['door_state_time_count']>95]\n",
    "    df_findperiod=df_findperiod.sort_values(by=['EM_mean'] , ascending=[True])#按照降序排序\n",
    "\n",
    "    time_end=time.time()\n",
    "    print('findperiod totally cost',datetime.timedelta(seconds=time_end-time_start))\n",
    "    print(df_findperiod)\n",
    "    period_df=df_findperiod.iloc[[0],[0]].values\n",
    "    for i in period_df:\n",
    "        for j in i:\n",
    "            period=j \n",
    "    print('most nomorl period is',period)\n",
    "    return period"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 主函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>E_Q</th>\n",
       "      <th>E_D</th>\n",
       "      <th>SPEED_SUM</th>\n",
       "      <th>SPEED_MAX_P</th>\n",
       "      <th>SPEED_MAX_N</th>\n",
       "      <th>SPEED_MAX_N_POSITION</th>\n",
       "      <th>START_POSITION</th>\n",
       "      <th>END_POSITION</th>\n",
       "      <th>IQ_SUM_P</th>\n",
       "      <th>IQ_SUM_N</th>\n",
       "      <th>ID_SUM_P</th>\n",
       "      <th>ID_SUM_N</th>\n",
       "      <th>EM</th>\n",
       "      <th>SPEED_SUM_SQR</th>\n",
       "      <th>SPEED_SUM_CNT</th>\n",
       "      <th>DOOR_STATE_TIME</th>\n",
       "      <th>DOOR_STATE</th>\n",
       "      <th>GATE</th>\n",
       "      <th>DOOR_STATE_Z</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HAPPENTIME</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2019-03-18 00:00:05</th>\n",
       "      <td>2</td>\n",
       "      <td>-1</td>\n",
       "      <td>460</td>\n",
       "      <td>60</td>\n",
       "      <td>-20</td>\n",
       "      <td>35745</td>\n",
       "      <td>35745</td>\n",
       "      <td>35745</td>\n",
       "      <td>62005</td>\n",
       "      <td>0</td>\n",
       "      <td>148</td>\n",
       "      <td>3192</td>\n",
       "      <td>-1</td>\n",
       "      <td>18000</td>\n",
       "      <td>39</td>\n",
       "      <td>39</td>\n",
       "      <td>0010001000000010</td>\n",
       "      <td>1</td>\n",
       "      <td>0010101000100010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-03-18 00:00:05</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>35745</td>\n",
       "      <td>35745</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>25315</td>\n",
       "      <td>0000001000100110</td>\n",
       "      <td>1</td>\n",
       "      <td>0010001000000010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-03-18 00:00:07</th>\n",
       "      <td>13</td>\n",
       "      <td>-1</td>\n",
       "      <td>-2104</td>\n",
       "      <td>0</td>\n",
       "      <td>-96</td>\n",
       "      <td>35719</td>\n",
       "      <td>35745</td>\n",
       "      <td>35587</td>\n",
       "      <td>241</td>\n",
       "      <td>493117</td>\n",
       "      <td>4794</td>\n",
       "      <td>5908</td>\n",
       "      <td>1</td>\n",
       "      <td>114448</td>\n",
       "      <td>45</td>\n",
       "      <td>45</td>\n",
       "      <td>0010011000100111</td>\n",
       "      <td>1</td>\n",
       "      <td>0000001000100110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-03-18 00:00:17</th>\n",
       "      <td>220</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1362</td>\n",
       "      <td>100</td>\n",
       "      <td>-576</td>\n",
       "      <td>22015</td>\n",
       "      <td>35587</td>\n",
       "      <td>16582</td>\n",
       "      <td>0</td>\n",
       "      <td>5472586</td>\n",
       "      <td>62669</td>\n",
       "      <td>63121</td>\n",
       "      <td>144</td>\n",
       "      <td>1023118</td>\n",
       "      <td>419</td>\n",
       "      <td>419</td>\n",
       "      <td>0010011100100101</td>\n",
       "      <td>1</td>\n",
       "      <td>0010011000100111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-03-18 00:00:34</th>\n",
       "      <td>7415</td>\n",
       "      <td>-2</td>\n",
       "      <td>-954075</td>\n",
       "      <td>164</td>\n",
       "      <td>-961</td>\n",
       "      <td>16582</td>\n",
       "      <td>16582</td>\n",
       "      <td>16582</td>\n",
       "      <td>0</td>\n",
       "      <td>512081249</td>\n",
       "      <td>22841</td>\n",
       "      <td>66662</td>\n",
       "      <td>-2</td>\n",
       "      <td>526484555</td>\n",
       "      <td>1821</td>\n",
       "      <td>1821</td>\n",
       "      <td>0010010100100001</td>\n",
       "      <td>1</td>\n",
       "      <td>0010011100100101</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      E_Q  E_D  SPEED_SUM  SPEED_MAX_P  SPEED_MAX_N  \\\n",
       "HAPPENTIME                                                            \n",
       "2019-03-18 00:00:05     2   -1        460           60          -20   \n",
       "2019-03-18 00:00:05     0    0          0            0            0   \n",
       "2019-03-18 00:00:07    13   -1      -2104            0          -96   \n",
       "2019-03-18 00:00:17   220   -1      -1362          100         -576   \n",
       "2019-03-18 00:00:34  7415   -2    -954075          164         -961   \n",
       "\n",
       "                     SPEED_MAX_N_POSITION  START_POSITION  END_POSITION  \\\n",
       "HAPPENTIME                                                                \n",
       "2019-03-18 00:00:05                 35745           35745         35745   \n",
       "2019-03-18 00:00:05                     0           35745         35745   \n",
       "2019-03-18 00:00:07                 35719           35745         35587   \n",
       "2019-03-18 00:00:17                 22015           35587         16582   \n",
       "2019-03-18 00:00:34                 16582           16582         16582   \n",
       "\n",
       "                     IQ_SUM_P   IQ_SUM_N  ID_SUM_P  ID_SUM_N   EM  \\\n",
       "HAPPENTIME                                                          \n",
       "2019-03-18 00:00:05     62005          0       148      3192   -1   \n",
       "2019-03-18 00:00:05         0          0         0         0    0   \n",
       "2019-03-18 00:00:07       241     493117      4794      5908    1   \n",
       "2019-03-18 00:00:17         0    5472586     62669     63121  144   \n",
       "2019-03-18 00:00:34         0  512081249     22841     66662   -2   \n",
       "\n",
       "                     SPEED_SUM_SQR  SPEED_SUM_CNT  DOOR_STATE_TIME  \\\n",
       "HAPPENTIME                                                           \n",
       "2019-03-18 00:00:05          18000             39               39   \n",
       "2019-03-18 00:00:05              0              0            25315   \n",
       "2019-03-18 00:00:07         114448             45               45   \n",
       "2019-03-18 00:00:17        1023118            419              419   \n",
       "2019-03-18 00:00:34      526484555           1821             1821   \n",
       "\n",
       "                           DOOR_STATE  GATE      DOOR_STATE_Z  \n",
       "HAPPENTIME                                                     \n",
       "2019-03-18 00:00:05  0010001000000010     1  0010101000100010  \n",
       "2019-03-18 00:00:05  0000001000100110     1  0010001000000010  \n",
       "2019-03-18 00:00:07  0010011000100111     1  0000001000100110  \n",
       "2019-03-18 00:00:17  0010011100100101     1  0010011000100111  \n",
       "2019-03-18 00:00:34  0010010100100001     1  0010011100100101  "
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#需输入门编号gate 寻优时间段period\n",
    "\n",
    "# #连接本地数据库\n",
    "# config = {'user': 'root',\n",
    "#             'passwd': 'ABCabc123',\n",
    "#             'host': '127.0.0.1',\n",
    "#             'db': 'maoqing'}\n",
    "#连接外部数据库\n",
    "config = {'user': 'root',\n",
    "             'passwd': 'ABCabc123',\n",
    "             'host': '10.84.1.20',\n",
    "             'db': 'maoqing'}\n",
    "database='door_data_can0812'\n",
    "\n",
    "sql_statement=\"select * from \"+database+\" where HAPPENTIME > '2019-03-18' and GATE = 1\"#and HAPPENTIME < '2019-04-10\n",
    "\n",
    "#def start():\n",
    "\n",
    "#链接数据库 拉一天的正常运行数据\n",
    "conn = sql.connect(**config)\n",
    "cur = conn.cursor(cursorclass=sql.cursors.DictCursor)#返回字段名\n",
    "#根据read_flag为null筛选出新录入的数据\n",
    "#cur.execute(\"select * from \"+database+\" where read_flag is null\")#door_data_can 为原始数据表名\n",
    "#data=cur.fetchall()#执行读取操作,data 为dict\n",
    "#rawdata_can = pd.DataFrame(list(data))#将读取结果转为dataframe格式\n",
    "rawdata_can=pd.read_sql(sql_statement,conn)\n",
    "rawdata_can['HAPPENTIME']=pd.to_datetime(rawdata_can['HAPPENTIME'])\n",
    "rawdata_can=rawdata_can.set_index('HAPPENTIME')\n",
    "rawdata_can=rawdata_can.loc['2019/3/12':,['E_Q', 'E_D', 'SPEED_SUM',  'SPEED_MAX_P',\n",
    "       'SPEED_MAX_N', 'SPEED_MAX_N_POSITION', 'START_POSITION', 'END_POSITION',\n",
    "       'IQ_SUM_P', 'IQ_SUM_N', 'ID_SUM_P', 'ID_SUM_N', 'EM','SPEED_SUM_SQR','SPEED_SUM_CNT','DOOR_STATE_TIME','DOOR_STATE','GATE','DOOR_STATE_Z']].copy()\n",
    "rawdata_can=rawdata_can.dropna()#删除有缺失值的行\n",
    "\n",
    "rawdata_can.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "door_gate 1\n"
     ]
    }
   ],
   "source": [
    "#统计读取出来的数据中有多少个门的数据\n",
    "rawdata_can_group=rawdata_can.groupby('GATE').count()\n",
    "for door_gate in rawdata_can_group.index:\n",
    "    print ('door_gate',door_gate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 对0号门数据进行寻优"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DOOR_STATE_TIME_mode 377\n",
      "findperiod totally cost 0:00:00.065855\n",
      "          time  door_state_time_count     EM_mean\n",
      "27  2019-04-08                     98  155.240612\n",
      "32  2019-04-13                     98  155.293463\n",
      "31  2019-04-12                     99  155.328929\n",
      "38  2019-04-19                     98  155.338665\n",
      "30  2019-04-11                     99  155.399861\n",
      "34  2019-04-15                     99  155.466620\n",
      "26  2019-04-07                     98  155.468011\n",
      "28  2019-04-09                     99  155.474600\n",
      "39  2019-04-20                     96  155.479833\n",
      "29  2019-04-10                     99  155.502782\n",
      "35  2019-04-16                     99  155.536857\n",
      "41  2019-04-22                     97  155.557719\n",
      "40  2019-04-21                     99  155.559110\n",
      "33  2019-04-14                     99  155.578874\n",
      "37  2019-04-18                     98  155.606398\n",
      "36  2019-04-17                     99  155.706333\n",
      "25  2019-04-06                     99  155.858136\n",
      "24  2019-04-05                    100  156.488178\n",
      "23  2019-04-04                    100  157.313901\n",
      "22  2019-04-03                    100  158.171766\n",
      "21  2019-04-02                    100  158.240779\n",
      "20  2019-04-01                     99  158.452174\n",
      "19  2019-03-31                    100  158.676164\n",
      "5   2019-03-17                     99  162.196106\n",
      "4   2019-03-16                     99  162.807505\n",
      "2   2019-03-14                     99  162.817929\n",
      "11  2019-03-23                     99  163.024339\n",
      "12  2019-03-24                    100  163.168867\n",
      "3   2019-03-15                     99  163.170257\n",
      "1   2019-03-13                     99  163.171071\n",
      "13  2019-03-25                     99  163.496873\n",
      "10  2019-03-22                     99  163.588317\n",
      "8   2019-03-20                     99  163.692147\n",
      "0   2019-03-12                     99  163.796386\n",
      "7   2019-03-19                     99  163.892286\n",
      "9   2019-03-21                     99  163.984017\n",
      "most nomorl period is 2019-04-08\n",
      "DOOR_STATE_TIME_mode 945\n",
      "findperiod totally cost 0:00:00.055850\n",
      "          time  door_state_time_count    EM_mean\n",
      "0   2019-03-12                     99   6.505907\n",
      "3   2019-03-15                     99   6.733658\n",
      "1   2019-03-13                     99   6.741307\n",
      "2   2019-03-14                     99   7.022949\n",
      "4   2019-03-16                     99   7.145935\n",
      "5   2019-03-17                     99   7.924896\n",
      "14  2019-03-26                     98   8.271242\n",
      "18  2019-04-01                     99   8.675243\n",
      "13  2019-03-25                     99   8.732267\n",
      "17  2019-03-31                     99   8.904729\n",
      "12  2019-03-24                     99   9.140473\n",
      "19  2019-04-02                     99   9.176634\n",
      "11  2019-03-23                     98   9.266342\n",
      "20  2019-04-03                     99   9.437413\n",
      "21  2019-04-04                     97   9.921525\n",
      "22  2019-04-05                     97  10.817803\n",
      "23  2019-04-06                     96  11.845619\n",
      "most nomorl period is 2019-03-12\n"
     ]
    }
   ],
   "source": [
    "#筛选开门数据\n",
    "# rawdata_can_open=rawdata_can[(rawdata_can['DOOR_STATE'].astype(str)=='0010011100100101')&(rawdata_can['GATE'].astype(str)==str(0))]  \n",
    "rawdata_can_open1=rawdata_can[(rawdata_can['DOOR_STATE'].astype(str)=='0010011100100101')&(rawdata_can['GATE'].astype(str)==str(0))].loc[:'2019-04-01',:]\n",
    "rawdata_can_open2=rawdata_can[(rawdata_can['DOOR_STATE'].astype(str)=='0010011100100101')&(rawdata_can['GATE'].astype(str)==str(0))&(rawdata_can['DOOR_STATE_Z'].astype(str)=='0010011000100111')].loc['2019-04-01':,:]\n",
    "rawdata_can_open=rawdata_can_open1.append(rawdata_can_open2)\n",
    "# rawdata_can_open=rawdata_can_open.loc['2019/3/12':,['E_Q', 'E_D', 'SPEED_SUM',  'SPEED_MAX_P',\n",
    "#    'SPEED_MAX_N', 'SPEED_MAX_N_POSITION', 'START_POSITION', 'END_POSITION',\n",
    "#    'IQ_SUM_P', 'IQ_SUM_N', 'ID_SUM_P', 'ID_SUM_N', 'EM','SPEED_SUM_SQR','SPEED_SUM_CNT','DOOR_STATE_TIME']].copy()\n",
    "# rawdata_can_open=rawdata_can_open.dropna()#删除缺失数据\n",
    "rawdata_can_open['SPEED_SUM_SQR_REAL'] = rawdata_can_open['SPEED_SUM_SQR']/rawdata_can_open['SPEED_SUM_CNT']\n",
    "rawdata_can_open=rawdata_can_open.drop(labels=['SPEED_SUM_SQR','SPEED_SUM_CNT','GATE','DOOR_STATE','DOOR_STATE_Z'],axis=1)#删除计算后的两列\n",
    "#print(rawdata_can_open.head())\n",
    "\n",
    "#筛选关门数据\n",
    "rawdata_can_close=rawdata_can[(rawdata_can['DOOR_STATE'].astype(str)=='0010101100101010')&(rawdata_can['GATE'].astype(str)==str(0))]  \n",
    "# rawdata_can_close=rawdata_can_close.loc['2019/3/12':,['E_Q', 'E_D', 'SPEED_SUM',  'SPEED_MAX_P',\n",
    "#    'SPEED_MAX_N', 'SPEED_MAX_N_POSITION', 'START_POSITION', 'END_POSITION',\n",
    "#    'IQ_SUM_P', 'IQ_SUM_N', 'ID_SUM_P', 'ID_SUM_N', 'EM','SPEED_SUM_SQR','SPEED_SUM_CNT','DOOR_STATE_TIME']].copy()\n",
    "# rawdata_can_close=rawdata_can_close.dropna()#删除缺失数据\n",
    "rawdata_can_close['SPEED_SUM_SQR_REAL'] = rawdata_can_close['SPEED_SUM_SQR']/rawdata_can_close['SPEED_SUM_CNT']\n",
    "rawdata_can_close=rawdata_can_close.drop(labels=['SPEED_SUM_SQR','SPEED_SUM_CNT','GATE','DOOR_STATE','DOOR_STATE_Z'],axis=1)#删除计算后的两列\n",
    "# print(rawdata_can_close.head())\n",
    "# rawdata_can_open.head()\n",
    "#rawdata_can_close.head()\n",
    "#进行正常周期筛选################################################################\n",
    "open_period=findbestperiod(rawdata_can_open)\n",
    "# open_period=\"2019-03-16\" \n",
    "rawdata_can_open=rawdata_can_open.drop(labels=['DOOR_STATE_TIME'],axis=1)#删除'DOOR_STATE_TIME'列\n",
    "rawdata_can_open=rawdata_can_open[open_period].copy()    \n",
    "#print(rawdata_can_open.head())\n",
    "#\n",
    "close_period=findbestperiod(rawdata_can_close)\n",
    "# close_period=\"2019-03-16\" \n",
    "rawdata_can_close=rawdata_can_close.drop(labels=['DOOR_STATE_TIME'],axis=1)#删除'DOOR_STATE_TIME'列\n",
    "rawdata_can_close=rawdata_can_close[close_period].copy()    \n",
    "#print(rawdata_can_close.head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "open_sample_len 50\n",
      "findopenbest totally cost 0:01:58.748694\n",
      "df_openbest                   time     a  count   cv.mean\n",
      "6  2019-04-08 00:07:10  1376   1375  0.949678\n",
      "26 2019-04-08 00:27:10  1376   1375  0.948461\n",
      "11 2019-04-08 00:12:10  1376   1375  0.948129\n",
      "39 2019-04-08 00:40:10  1376   1375  0.946094\n",
      "3  2019-04-08 00:04:10  1376   1375  0.941176\n",
      "[['2019-04-08T00:07:10.000000000']]\n",
      "2019-04-08T00:07:10.000000000\n",
      "close_sample_len 50\n",
      "findclosebest totally cost 0:01:54.823884\n",
      "df_closebest                   time     a  count   cv.mean\n",
      "6  2019-03-12 00:07:43  1336   1336  0.945736\n",
      "7  2019-03-12 00:08:43  1336   1336  0.944998\n",
      "3  2019-03-12 00:04:43  1336   1336  0.942659\n",
      "31 2019-03-12 00:33:42  1336   1336  0.941164\n",
      "44 2019-03-12 00:46:42  1336   1336  0.941027\n",
      "2019-03-12T00:07:43.000000000\n",
      "findbest totally cost 0:03:53.585546\n"
     ]
    }
   ],
   "source": [
    "###############################################################################    \n",
    "#去除离群点数据\n",
    "data_can_open=filter_data(rawdata_can_open)\n",
    "data_can_close=filter_data(rawdata_can_close)\n",
    "data_can_open.head()\n",
    "###############################################################################\n",
    "#去除离群点后的数据,对所有的特征做标准化化处理,并保存标准化模型\n",
    "data_st1_sd_sps = open_StandardScaler(data_can_open,a_open_sd_model_flie)\n",
    "data_st2_sd_sps = close_StandardScaler(data_can_close,a_close_sd_model_flie)\n",
    "data_st1_sd_sps.head()\n",
    "\n",
    "###############################################################################\n",
    "#启动线程对开门和关门数据进行寻优并保存最优的模型\n",
    "time_start=time.time()\n",
    "threading.Thread(target=opendata_findbest(data_st1_sd_sps,a_open_normal_model_flie)).start()\n",
    "threading.Thread(target=closedata_findbest(data_st2_sd_sps,a_close_normal_model_flie)).start()\n",
    "time_end=time.time()\n",
    "print('findbest totally cost',datetime.timedelta(seconds=time_end-time_start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 对1号门数据进行寻优"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DOOR_STATE_TIME_mode 421\n",
      "findperiod totally cost 0:00:00.166554\n",
      "           time  door_state_time_count     EM_mean\n",
      "37   2019-04-25                    100  195.954798\n",
      "44   2019-05-02                    100  195.982615\n",
      "117  2019-07-14                    100  196.425990\n",
      "38   2019-04-26                    100  196.705146\n",
      "118  2019-07-15                    100  196.888039\n",
      "43   2019-05-01                    100  197.551460\n",
      "116  2019-07-13                    100  197.594576\n",
      "115  2019-07-12                    100  197.717663\n",
      "127  2019-07-24                    100  197.753129\n",
      "128  2019-07-25                    100  197.854760\n",
      "59   2019-05-17                    100  198.010431\n",
      "36   2019-04-24                    100  198.160640\n",
      "120  2019-07-17                    100  198.399861\n",
      "62   2019-05-20                    100  198.408901\n",
      "132  2019-07-29                    100  198.601530\n",
      "60   2019-05-18                    100  198.849096\n",
      "102  2019-06-29                    100  198.901947\n",
      "58   2019-05-16                    100  199.061892\n",
      "126  2019-07-23                    100  199.070932\n",
      "143  2019-08-09                    100  199.075104\n",
      "134  2019-07-31                    100  199.141168\n",
      "106  2019-07-03                    100  199.143950\n",
      "107  2019-07-04                    100  199.200278\n",
      "124  2019-07-21                    100  199.212100\n",
      "109  2019-07-06                    100  199.234353\n",
      "130  2019-07-27                    100  199.243394\n",
      "133  2019-07-30                    100  199.255216\n",
      "114  2019-07-11                    100  199.256606\n",
      "125  2019-07-22                    100  199.256606\n",
      "119  2019-07-16                    100  199.302503\n",
      "..          ...                    ...         ...\n",
      "3    2019-03-22                     99  206.650904\n",
      "16   2019-04-04                    100  206.692480\n",
      "40   2019-04-28                     99  206.801808\n",
      "73   2019-05-31                    100  206.883866\n",
      "77   2019-06-04                    100  207.100139\n",
      "13   2019-04-01                     98  207.166203\n",
      "15   2019-04-03                    100  207.248957\n",
      "17   2019-04-05                     99  207.523644\n",
      "14   2019-04-02                    100  207.636996\n",
      "86   2019-06-13                     99  207.656467\n",
      "88   2019-06-15                     99  207.821975\n",
      "29   2019-04-17                     99  207.876217\n",
      "84   2019-06-11                    100  208.151599\n",
      "8    2019-03-27                    100  208.223922\n",
      "89   2019-06-16                     99  208.364395\n",
      "68   2019-05-26                    100  208.583449\n",
      "82   2019-06-09                    100  208.655772\n",
      "26   2019-04-14                    100  208.699583\n",
      "91   2019-06-18                     99  209.044506\n",
      "0    2019-03-19                     99  209.098053\n",
      "87   2019-06-14                     98  209.332406\n",
      "7    2019-03-26                     98  209.842837\n",
      "4    2019-03-23                     98  209.890125\n",
      "1    2019-03-20                    100  210.295549\n",
      "67   2019-05-25                    100  210.644645\n",
      "78   2019-06-05                    100  210.684979\n",
      "83   2019-06-10                     99  210.708623\n",
      "6    2019-03-25                     96  210.946453\n",
      "79   2019-06-06                     99  210.952712\n",
      "5    2019-03-24                     96  211.874131\n",
      "\n",
      "[144 rows x 3 columns]\n",
      "most nomorl period is 2019-04-25\n",
      "DOOR_STATE_TIME_mode 983\n",
      "findperiod totally cost 0:00:00.167553\n",
      "           time  door_state_time_count    EM_mean\n",
      "117  2019-07-14                    100  37.952712\n",
      "118  2019-07-15                    100  38.351878\n",
      "115  2019-07-12                     97  38.737830\n",
      "116  2019-07-13                    100  39.082754\n",
      "120  2019-07-17                     98  40.173853\n",
      "106  2019-07-03                    100  40.494437\n",
      "127  2019-07-24                     98  40.724618\n",
      "103  2019-06-30                    100  40.972184\n",
      "114  2019-07-11                     99  41.059805\n",
      "59   2019-05-17                     99  41.111266\n",
      "126  2019-07-23                     97  41.209318\n",
      "125  2019-07-22                     96  41.211405\n",
      "104  2019-07-01                    100  41.258693\n",
      "102  2019-06-29                    100  41.260083\n",
      "128  2019-07-25                     98  41.323366\n",
      "124  2019-07-21                     99  41.404729\n",
      "107  2019-07-04                    100  41.460362\n",
      "143  2019-08-09                     99  41.813630\n",
      "119  2019-07-16                     98  42.036857\n",
      "123  2019-07-20                     98  42.102225\n",
      "109  2019-07-06                     99  42.246175\n",
      "60   2019-05-18                    100  42.358832\n",
      "110  2019-07-07                    100  42.538568\n",
      "132  2019-07-29                     96  42.552851\n",
      "101  2019-06-28                    100  42.573018\n",
      "58   2019-05-16                     99  42.621001\n",
      "94   2019-06-21                    100  42.650209\n",
      "100  2019-06-27                    100  42.725313\n",
      "105  2019-07-02                     99  43.139082\n",
      "62   2019-05-20                     99  43.300417\n",
      "..          ...                    ...        ...\n",
      "93   2019-06-20                    100  45.002086\n",
      "97   2019-06-24                    100  45.102921\n",
      "99   2019-06-26                     99  45.581363\n",
      "50   2019-05-08                     99  45.629346\n",
      "49   2019-05-07                     99  46.066759\n",
      "53   2019-05-11                     97  46.318498\n",
      "98   2019-06-25                    100  46.567755\n",
      "54   2019-05-12                     99  46.771210\n",
      "61   2019-05-19                     99  46.887344\n",
      "57   2019-05-15                     99  46.956885\n",
      "48   2019-05-06                     96  47.492350\n",
      "70   2019-05-28                    100  47.544506\n",
      "52   2019-05-10                    100  47.643255\n",
      "51   2019-05-09                     99  48.061892\n",
      "75   2019-06-02                    100  48.210014\n",
      "63   2019-05-21                    100  48.270703\n",
      "64   2019-05-22                    100  49.059110\n",
      "55   2019-05-13                     99  49.504172\n",
      "80   2019-06-07                     98  49.676634\n",
      "81   2019-06-08                     98  49.877608\n",
      "66   2019-05-24                     99  49.957580\n",
      "56   2019-05-14                     99  50.278860\n",
      "72   2019-05-30                     99  50.799722\n",
      "71   2019-05-29                    100  50.958971\n",
      "65   2019-05-23                     99  51.181502\n",
      "76   2019-06-03                     97  51.239221\n",
      "74   2019-06-01                    100  51.243394\n",
      "77   2019-06-04                     98  51.608484\n",
      "85   2019-06-12                     99  51.746175\n",
      "68   2019-05-26                     99  52.043115\n",
      "\n",
      "[70 rows x 3 columns]\n",
      "most nomorl period is 2019-07-14\n"
     ]
    }
   ],
   "source": [
    "#筛选开门数据\n",
    "rawdata_can_open=rawdata_can[(rawdata_can['DOOR_STATE'].astype(str)=='0010011100100101')&(rawdata_can['GATE'].astype(str)==str(1))]  \n",
    "rawdata_can_open=rawdata_can_open.loc['2019/3/19':,:].copy()#限制寻优范围\n",
    "# rawdata_can_open=rawdata_can_open.dropna()#删除缺失数据\n",
    "rawdata_can_open['SPEED_SUM_SQR_REAL'] = rawdata_can_open['SPEED_SUM_SQR']/rawdata_can_open['SPEED_SUM_CNT']\n",
    "rawdata_can_open=rawdata_can_open.drop(labels=['SPEED_SUM_SQR','SPEED_SUM_CNT','GATE','DOOR_STATE','DOOR_STATE_Z'],axis=1)#删除计算后的两列\n",
    "#print(rawdata_can_open.head())\n",
    "\n",
    "#筛选关门数据\n",
    "rawdata_can_close=rawdata_can[(rawdata_can['DOOR_STATE'].astype(str)=='0010101100101010')&(rawdata_can['GATE'].astype(str)==str(1))]  \n",
    "rawdata_can_close=rawdata_can_close.loc['2019/3/19':,:].copy()#限制寻优范围\n",
    "# rawdata_can_close=rawdata_can_close.dropna()#删除缺失数据\n",
    "rawdata_can_close['SPEED_SUM_SQR_REAL'] = rawdata_can_close['SPEED_SUM_SQR']/rawdata_can_close['SPEED_SUM_CNT']\n",
    "rawdata_can_close=rawdata_can_close.drop(labels=['SPEED_SUM_SQR','SPEED_SUM_CNT','GATE','DOOR_STATE','DOOR_STATE_Z'],axis=1)#删除计算后的两列\n",
    "#print(rawdata_can_close.head())\n",
    "rawdata_can_open.head()\n",
    "#rawdata_can_close.head()\n",
    "#进行正常周期筛选################################################################\n",
    "open_period=findbestperiod(rawdata_can_open)\n",
    "close_period=findbestperiod(rawdata_can_close)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "#截取正常运行 ################################################################\n",
    "open_period=\"2019-05-01\" \n",
    "rawdata_can_open_cut=rawdata_can_open.drop(labels=['DOOR_STATE_TIME'],axis=1)#删除'DOOR_STATE_TIME'列\n",
    "rawdata_can_open_cut=rawdata_can_open_cut[open_period].copy()    \n",
    "#print(rawdata_can_open.head())\n",
    "\n",
    "close_period=\"2019-05-02\" \n",
    "rawdata_can_close_cut=rawdata_can_close.drop(labels=['DOOR_STATE_TIME'],axis=1)#删除'DOOR_STATE_TIME'列\n",
    "rawdata_can_close_cut=rawdata_can_close_cut[close_period].copy()    \n",
    "#print(rawdata_can_close.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>E_Q</th>\n",
       "      <th>E_D</th>\n",
       "      <th>SPEED_SUM</th>\n",
       "      <th>SPEED_MAX_P</th>\n",
       "      <th>SPEED_MAX_N</th>\n",
       "      <th>SPEED_MAX_N_POSITION</th>\n",
       "      <th>START_POSITION</th>\n",
       "      <th>END_POSITION</th>\n",
       "      <th>IQ_SUM_P</th>\n",
       "      <th>IQ_SUM_N</th>\n",
       "      <th>ID_SUM_P</th>\n",
       "      <th>ID_SUM_N</th>\n",
       "      <th>EM</th>\n",
       "      <th>SPEED_SUM_SQR_REAL</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HAPPENTIME</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2019-05-01 00:00:04</th>\n",
       "      <td>-0.004016</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-2.072182</td>\n",
       "      <td>2.257055</td>\n",
       "      <td>-2.359357</td>\n",
       "      <td>1.732430</td>\n",
       "      <td>1.441696</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.267363</td>\n",
       "      <td>0.725407</td>\n",
       "      <td>0.470552</td>\n",
       "      <td>1.364754</td>\n",
       "      <td>1.592845</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-05-01 00:01:04</th>\n",
       "      <td>0.285552</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.086970</td>\n",
       "      <td>-0.315141</td>\n",
       "      <td>0.297708</td>\n",
       "      <td>0.869992</td>\n",
       "      <td>1.441696</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.789679</td>\n",
       "      <td>-0.269839</td>\n",
       "      <td>-1.419021</td>\n",
       "      <td>1.364754</td>\n",
       "      <td>-1.265394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-05-01 00:02:04</th>\n",
       "      <td>-0.293584</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.579576</td>\n",
       "      <td>-0.315141</td>\n",
       "      <td>0.297708</td>\n",
       "      <td>0.869992</td>\n",
       "      <td>-0.227893</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.847931</td>\n",
       "      <td>1.424225</td>\n",
       "      <td>1.456757</td>\n",
       "      <td>1.364754</td>\n",
       "      <td>-1.589458</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-05-01 00:03:04</th>\n",
       "      <td>-0.872721</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-2.072182</td>\n",
       "      <td>-1.086800</td>\n",
       "      <td>0.297708</td>\n",
       "      <td>1.732430</td>\n",
       "      <td>-0.227893</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.642077</td>\n",
       "      <td>0.550265</td>\n",
       "      <td>0.971078</td>\n",
       "      <td>0.272632</td>\n",
       "      <td>-1.398909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-05-01 00:05:04</th>\n",
       "      <td>-0.872721</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.579576</td>\n",
       "      <td>-0.315141</td>\n",
       "      <td>-1.030825</td>\n",
       "      <td>0.007554</td>\n",
       "      <td>-1.897481</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.582627</td>\n",
       "      <td>-0.764178</td>\n",
       "      <td>0.420148</td>\n",
       "      <td>0.272632</td>\n",
       "      <td>0.668616</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          E_Q  E_D  SPEED_SUM  SPEED_MAX_P  SPEED_MAX_N  \\\n",
       "HAPPENTIME                                                                \n",
       "2019-05-01 00:00:04 -0.004016  0.0  -2.072182     2.257055    -2.359357   \n",
       "2019-05-01 00:01:04  0.285552  0.0  -1.086970    -0.315141     0.297708   \n",
       "2019-05-01 00:02:04 -0.293584  0.0  -1.579576    -0.315141     0.297708   \n",
       "2019-05-01 00:03:04 -0.872721  0.0  -2.072182    -1.086800     0.297708   \n",
       "2019-05-01 00:05:04 -0.872721  0.0  -1.579576    -0.315141    -1.030825   \n",
       "\n",
       "                     SPEED_MAX_N_POSITION  START_POSITION  END_POSITION  \\\n",
       "HAPPENTIME                                                                \n",
       "2019-05-01 00:00:04              1.732430        1.441696           0.0   \n",
       "2019-05-01 00:01:04              0.869992        1.441696           0.0   \n",
       "2019-05-01 00:02:04              0.869992       -0.227893           0.0   \n",
       "2019-05-01 00:03:04              1.732430       -0.227893           0.0   \n",
       "2019-05-01 00:05:04              0.007554       -1.897481           0.0   \n",
       "\n",
       "                     IQ_SUM_P  IQ_SUM_N  ID_SUM_P  ID_SUM_N        EM  \\\n",
       "HAPPENTIME                                                              \n",
       "2019-05-01 00:00:04       0.0  1.267363  0.725407  0.470552  1.364754   \n",
       "2019-05-01 00:01:04       0.0  0.789679 -0.269839 -1.419021  1.364754   \n",
       "2019-05-01 00:02:04       0.0  0.847931  1.424225  1.456757  1.364754   \n",
       "2019-05-01 00:03:04       0.0  0.642077  0.550265  0.971078  0.272632   \n",
       "2019-05-01 00:05:04       0.0  0.582627 -0.764178  0.420148  0.272632   \n",
       "\n",
       "                     SPEED_SUM_SQR_REAL  \n",
       "HAPPENTIME                               \n",
       "2019-05-01 00:00:04            1.592845  \n",
       "2019-05-01 00:01:04           -1.265394  \n",
       "2019-05-01 00:02:04           -1.589458  \n",
       "2019-05-01 00:03:04           -1.398909  \n",
       "2019-05-01 00:05:04            0.668616  "
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "###############################################################################    \n",
    "#去除离群点数据\n",
    "data_can_open_cut=filter_data(rawdata_can_open_cut)\n",
    "data_can_close_cut=filter_data(rawdata_can_close_cut)\n",
    "\n",
    "###############################################################################\n",
    "#去除离群点后的数据,对所有的特征做标准化化处理,并保存标准化模型\n",
    "data_st1_sd_sps = open_StandardScaler(data_can_open_cut,b_open_sd_model_flie)\n",
    "data_st2_sd_sps = close_StandardScaler(data_can_close_cut,b_close_sd_model_flie)\n",
    "data_st1_sd_sps.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "close_sample_len 50\n",
      "findclosebest totally cost 0:01:51.910910\n",
      "df_closebest                   time     a  count   cv.mean\n",
      "35 2019-05-02 00:49:32  1260   1260  0.929659\n",
      "43 2019-05-02 00:57:32  1260   1260  0.928956\n",
      "17 2019-05-02 00:26:32  1260   1260  0.928875\n",
      "31 2019-05-02 00:45:32  1260   1260  0.928279\n",
      "44 2019-05-02 00:58:32  1260   1260  0.927227\n",
      "2019-05-02T00:49:32.000000000\n",
      "findbest totally cost 0:01:51.916894\n"
     ]
    }
   ],
   "source": [
    "###############################################################################\n",
    "#启动线程对开门和关门数据进行寻优并保存最优的模型\n",
    "time_start=time.time()\n",
    "# threading.Thread(target=opendata_findbest(data_st1_sd_sps,b_open_normal_model_flie)).start()#开门模型训练保存\n",
    "threading.Thread(target=closedata_findbest(data_st2_sd_sps,b_close_normal_model_flie)).start()#关门模型训练保存\n",
    "time_end=time.time()\n",
    "print('findbest totally cost',datetime.timedelta(seconds=time_end-time_start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 测试用，保存normal模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data_st1_sd_sps' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-14-97ccaa3c4d6f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#保存指定的关门模型\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mdata_s1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata_st1_sd_sps\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'2019-04-08 00:46:45'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mdata_s1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_s1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mnormal_obj\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgmm_clustering\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdata_s1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m#保存normal_obj模型\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'data_st1_sd_sps' is not defined"
     ]
    }
   ],
   "source": [
    "#保存指定的开门模型\n",
    "data_s1 = data_st1_sd_sps.loc['2019-04-08 00:46:45'].copy()\n",
    "data_s1 = pd.DataFrame(data_s1)\n",
    "normal_obj = gmm_clustering(1,data_s1)\n",
    "#保存normal_obj模型\n",
    "#保存训练好的模型文件\n",
    "joblib.dump(normal_obj, 'onlycan_opendata_normal_obj_model1.pkl',compress=3)\n",
    "#导入训练好的模型文件\n",
    "clf = joblib.load('onlycan_opendata_normal_obj_model1.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
